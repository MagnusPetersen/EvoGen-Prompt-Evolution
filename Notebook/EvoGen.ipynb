{"cells":[{"cell_type":"markdown","metadata":{"id":"c442uQJ_gUgy"},"source":["# **Evolutionary Prompt-Mining based on Deforum Stable Diffusion v0.2**\n","[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n","\n","The aesthetics model that is an integral part of this method was made by [Katherine Crowson](https://twitter.com/RiversHaveWings) and can be found on her [Github account](https://github.com/crowsonkb/simulacra-aesthetic-models). \n","\n","Notebook by [Magnus Petersen](https://twitter.com/Omorfiamorphism), the baseline of the notebook, setup, description, and image generation, is based on the\n","[deforum](https://discord.gg/upmXXsrwZc) notebook."]},{"cell_type":"markdown","metadata":{"id":"T4knibRpAQ06"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2g-f7cQmf2Nt"},"outputs":[],"source":["#@markdown **NVIDIA GPU**\n","import subprocess\n","sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n","print(sub_p_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"TxIOPT0G5Lx1"},"outputs":[],"source":["#@markdown **Model and Output Paths**\n","# ask for the link\n","print(\"Local Path Variables:\\n\")\n","\n","models_path = \"/content/models\" #@param {type:\"string\"}\n","output_path = \"/content/output\" #@param {type:\"string\"}\n","\n","#@markdown **Google Drive Path Variables (Optional)**\n","mount_google_drive = True #@param {type:\"boolean\"}\n","force_remount = False\n","\n","if mount_google_drive:\n","    from google.colab import drive # type: ignore\n","    try:\n","        drive_path = \"/content/drive\"\n","        drive.mount(drive_path,force_remount=force_remount)\n","        models_path_gdrive = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n","        output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n","        models_path = models_path_gdrive\n","        output_path = output_path_gdrive\n","    except:\n","        print(\"...error mounting drive or with drive path variables\")\n","        print(\"...reverting to default path variables\")\n","\n","import os\n","os.makedirs(models_path, exist_ok=True)\n","os.makedirs(output_path, exist_ok=True)\n","\n","print(f\"models_path: {models_path}\")\n","print(f\"output_path: {output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VRNl2mfepEIe"},"outputs":[],"source":["#@markdown **Setup Environment**\n","\n","setup_environment = True #@param {type:\"boolean\"}\n","print_subprocess = False #@param {type:\"boolean\"}\n","\n","if setup_environment:\n","    import subprocess, time\n","    print(\"Setting up environment...\")\n","    start_time = time.time()\n","    all_process = [\n","        ['pip', 'install', 'torch==1.12.1+cu113', 'torchvision==0.13.1+cu113', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'],\n","        ['pip', 'install', 'omegaconf==2.2.3', 'einops==0.4.1', 'pytorch-lightning==1.7.4', 'torchmetrics==0.9.3', 'torchtext==0.13.1', 'transformers==4.21.2', 'kornia==0.6.7'],\n","        ['git', 'clone', 'https://github.com/deforum/stable-diffusion'],\n","        ['pip', 'install', '-e', 'git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers'],\n","        ['pip', 'install', '-e', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n","        ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq'],\n","        ['git', 'clone', 'https://github.com/shariqfarooq123/AdaBins.git'],\n","        ['git', 'clone', 'https://github.com/isl-org/MiDaS.git'],\n","        ['git', 'clone', 'https://github.com/MSFTserver/pytorch3d-lite.git'],\n","    ]\n","    for process in all_process:\n","        running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n","        if print_subprocess:\n","            print(running)\n","    \n","    print(subprocess.run(['git', 'clone', 'https://github.com/deforum/k-diffusion/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n","    with open('k-diffusion/k_diffusion/__init__.py', 'w') as f:\n","        f.write('')\n","\n","    end_time = time.time()\n","    print(f\"Environment set up in {end_time-start_time:.0f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"81qmVZbrm4uu"},"outputs":[],"source":["#@markdown **Python Definitions**\n","import json\n","from IPython import display\n","\n","import gc, math, os, pathlib, subprocess, sys, time\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","import requests\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as TF\n","from contextlib import contextmanager, nullcontext\n","from einops import rearrange, repeat\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from pytorch_lightning import seed_everything\n","from skimage.exposure import match_histograms\n","from torchvision.utils import make_grid\n","from tqdm import tqdm, trange\n","from types import SimpleNamespace\n","from torch import autocast\n","\n","sys.path.extend([\n","    'src/taming-transformers',\n","    'src/clip',\n","    'stable-diffusion/',\n","    'k-diffusion',\n","    'pytorch3d-lite',\n","    'AdaBins',\n","    'MiDaS',\n","])\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","! git clone https://github.com/MagnusPetersen/EvoGen-Prompt-Evolution.git\n","\n","from helpers import save_samples, sampler_fn\n","from k_diffusion.external import CompVisDenoiser\n","from ldm.util import instantiate_from_config\n","from ldm.models.diffusion.ddim import DDIMSampler\n","from ldm.models.diffusion.plms import PLMSSampler\n","\n","class CFGDenoiser(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.inner_model = model\n","\n","    def forward(self, x, sigma, uncond, cond, cond_scale):\n","        x_in = torch.cat([x] * 2)\n","        sigma_in = torch.cat([sigma] * 2)\n","        cond_in = torch.cat([uncond, cond])\n","        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n","        return uncond + (cond - uncond) * cond_scale\n","\n","def add_noise(sample: torch.Tensor, noise_amt: float):\n","    return sample + torch.randn(sample.shape, device=sample.device) * noise_amt\n","\n","def get_output_folder(output_path, batch_folder):\n","    out_path = os.path.join(output_path,time.strftime('%Y-%m'))\n","    if batch_folder != \"\":\n","        out_path = os.path.join(out_path, batch_folder)\n","    os.makedirs(out_path, exist_ok=True)\n","    return out_path\n","\n","def load_img(path, shape):\n","    if path.startswith('http://') or path.startswith('https://'):\n","        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n","    else:\n","        image = Image.open(path).convert('RGB')\n","\n","    image = image.resize(shape, resample=Image.LANCZOS)\n","    image = np.array(image).astype(np.float16) / 255.0\n","    image = image[None].transpose(0, 3, 1, 2)\n","    image = torch.from_numpy(image)\n","    return 2.*image - 1.\n","\n","def maintain_colors(prev_img, color_match_sample, mode):\n","    if mode == 'Match Frame 0 RGB':\n","        return match_histograms(prev_img, color_match_sample, multichannel=True)\n","    elif mode == 'Match Frame 0 HSV':\n","        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n","        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n","        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n","        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n","    else: # Match Frame 0 LAB\n","        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n","        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n","        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n","        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)\n","\n","def make_callback(sampler, dynamic_threshold=None, static_threshold=None):  \n","    # Creates the callback function to be passed into the samplers\n","    # The callback function is applied to the image after each step\n","    def dynamic_thresholding_(img, threshold):\n","        # Dynamic thresholding from Imagen paper (May 2022)\n","        s = np.percentile(np.abs(img.cpu()), threshold, axis=tuple(range(1,img.ndim)))\n","        s = np.max(np.append(s,1.0))\n","        torch.clamp_(img, -1*s, s)\n","        torch.FloatTensor.div_(img, s)\n","\n","    # Callback for samplers in the k-diffusion repo, called thus:\n","    #   callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n","    def k_callback(args_dict):\n","        if static_threshold is not None:\n","            torch.clamp_(args_dict['x'], -1*static_threshold, static_threshold)\n","        if dynamic_threshold is not None:\n","            dynamic_thresholding_(args_dict['x'], dynamic_threshold)\n","\n","    # Function that is called on the image (img) and step (i) at each step\n","    def img_callback(img, i):\n","        # Thresholding functions\n","        if dynamic_threshold is not None:\n","            dynamic_thresholding_(img, dynamic_threshold)\n","        if static_threshold is not None:\n","            torch.clamp_(img, -1*static_threshold, static_threshold)\n","\n","    if sampler in [\"plms\",\"ddim\"]: \n","        # Callback function formated for compvis latent diffusion samplers\n","        callback = img_callback\n","    else: \n","        # Default callback function uses k-diffusion sampler variables\n","        callback = k_callback\n","\n","    return callback\n","\n","def generate(args, prompt_batch, return_latent=False, return_sample=False, return_c=False):\n","    seed_everything(args.seed)\n","    os.makedirs(args.outdir, exist_ok=True)\n","\n","    if args.sampler == 'plms':\n","        sampler = PLMSSampler(model)\n","    else:\n","        sampler = DDIMSampler(model)\n","\n","    model_wrap = CompVisDenoiser(model)       \n","    batch_size = args.n_samples\n","    data = [prompt_batch]\n","\n","    init_latent = None\n","    if args.init_latent is not None:\n","        init_latent = args.init_latent\n","    elif args.init_sample is not None:\n","        init_latent = model.get_first_stage_encoding(model.encode_first_stage(args.init_sample))\n","    elif args.init_image != None and args.init_image != '':\n","        init_image = load_img(args.init_image, shape=(args.W, args.H)).to(device)\n","        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n","        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space        \n","\n","    sampler.make_schedule(ddim_num_steps=args.steps, ddim_eta=args.ddim_eta, verbose=False)\n","\n","    t_enc = int((1.0-args.strength) * args.steps)\n","\n","    start_code = None\n","    if args.fixed_code and init_latent == None:\n","        start_code = torch.randn([args.n_samples, args.C, args.H // args.f, args.W // args.f], device=device)\n","\n","    callback = make_callback(sampler=args.sampler,\n","                            dynamic_threshold=args.dynamic_threshold, \n","                            static_threshold=args.static_threshold)\n","\n","    results = []\n","    precision_scope = autocast if args.precision == \"autocast\" else nullcontext\n","    with torch.no_grad():\n","        with precision_scope(\"cuda\"):\n","            with model.ema_scope():\n","                for prompts in data:\n","                    uc = None\n","                    if args.scale != 1.0:\n","                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n","                    if isinstance(prompts, tuple):\n","                        prompts = list(prompts)\n","                    c = model.get_learned_conditioning(prompts)\n","\n","                    if args.init_c != None:\n","                        c = args.init_c\n","\n","                    if args.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n","                        samples = sampler_fn(\n","                            c=c, \n","                            uc=uc, \n","                            args=args, \n","                            model_wrap=model_wrap, \n","                            init_latent=init_latent, \n","                            t_enc=t_enc, \n","                            device=device, \n","                            cb=callback)\n","                    else:\n","\n","                        if init_latent != None:\n","                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n","                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.scale,\n","                                                    unconditional_conditioning=uc,)\n","                        else:\n","                            if args.sampler == 'plms' or args.sampler == 'ddim':\n","                                shape = [args.C, args.H // args.f, args.W // args.f]\n","                                samples, _ = sampler.sample(S=args.steps,\n","                                                                conditioning=c,\n","                                                                batch_size=args.n_samples,\n","                                                                shape=shape,\n","                                                                verbose=False,\n","                                                                unconditional_guidance_scale=args.scale,\n","                                                                unconditional_conditioning=uc,\n","                                                                eta=args.ddim_eta,\n","                                                                x_T=start_code,\n","                                                                img_callback=callback)\n","\n","                    if return_latent:\n","                        results.append(samples.clone())\n","\n","                    x_samples = model.decode_first_stage(samples)\n","                    if return_sample:\n","                        results.append(x_samples.clone())\n","\n","                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n","\n","                    if return_c:\n","                        results.append(c.clone())\n","\n","                    for x_sample in x_samples:\n","                    #    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n","                    #    image = Image.fromarray(x_sample.astype(np.uint8))\n","                        results.append(x_sample)\n","    return results\n","\n","def sample_from_cv2(sample: np.ndarray) -> torch.Tensor:\n","    sample = ((sample.astype(float) / 255.0) * 2) - 1\n","    sample = sample[None].transpose(0, 3, 1, 2).astype(np.float16)\n","    sample = torch.from_numpy(sample)\n","    return sample\n","\n","def sample_to_cv2(sample: torch.Tensor) -> np.ndarray:\n","    sample_f32 = rearrange(sample.squeeze().cpu().numpy(), \"c h w -> h w c\").astype(np.float32)\n","    sample_f32 = ((sample_f32 * 0.5) + 0.5).clip(0, 1)\n","    sample_int8 = (sample_f32 * 255).astype(np.uint8)\n","    return sample_int8"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CIUJ7lWI4v53"},"outputs":[],"source":["#@markdown **Select and Load Model**\n","from IPython.display import clear_output, display\n","\n","model_config = \"v1-inference.yaml\" #@param [\"custom\",\"v1-inference.yaml\"]\n","model_checkpoint =  \"sd-v1-4.ckpt\" #@param [\"custom\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\"]\n","custom_config_path = \"\" #@param {type:\"string\"}\n","custom_checkpoint_path = \"\" #@param {type:\"string\"}\n","\n","check_sha256 = True #@param {type:\"boolean\"}\n","\n","load_on_run_all = True #@param {type: 'boolean'}\n","half_precision = True # needs to be fixed\n","\n","model_map = {\n","    \"sd-v1-4-full-ema.ckpt\": {'sha256': '14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a'},\n","    \"sd-v1-4.ckpt\": {'sha256': 'fe4efff1e174c627256e44ec2991ba279b3816e364b49f9be2abc0b3ff3f8556'},\n","    \"sd-v1-3-full-ema.ckpt\": {'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca'},\n","    \"sd-v1-3.ckpt\": {'sha256': '2cff93af4dcc07c3e03110205988ff98481e86539c51a8098d4f2236e41f7f2f'},\n","    \"sd-v1-2-full-ema.ckpt\": {'sha256': 'bc5086a904d7b9d13d2a7bccf38f089824755be7261c7399d92e555e1e9ac69a'},\n","    \"sd-v1-2.ckpt\": {'sha256': '3b87d30facd5bafca1cbed71cfb86648aad75d1c264663c0cc78c7aea8daec0d'},\n","    \"sd-v1-1-full-ema.ckpt\": {'sha256': 'efdeb5dc418a025d9a8cc0a8617e106c69044bc2925abecc8a254b2910d69829'},\n","    \"sd-v1-1.ckpt\": {'sha256': '86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea'}\n","}\n","\n","# config path\n","ckpt_config_path = custom_config_path if model_config == \"custom\" else os.path.join(models_path, model_config)\n","if os.path.exists(ckpt_config_path):\n","    print(f\"{ckpt_config_path} exists\")\n","else:\n","    ckpt_config_path = \"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n","print(f\"Using config: {ckpt_config_path}\")\n","\n","# checkpoint path or download\n","ckpt_path = custom_checkpoint_path if model_checkpoint == \"custom\" else os.path.join(models_path, model_checkpoint)\n","ckpt_valid = True\n","if os.path.exists(ckpt_path):\n","    print(f\"{ckpt_path} exists\")\n","else:\n","    print(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n","    ckpt_valid = False\n","\n","if check_sha256 and model_checkpoint != \"custom\" and ckpt_valid:\n","    import hashlib\n","    print(\"\\n...checking sha256\")\n","    with open(ckpt_path, \"rb\") as f:\n","        bytes = f.read() \n","        hash = hashlib.sha256(bytes).hexdigest()\n","        del bytes\n","    if model_map[model_checkpoint][\"sha256\"] == hash:\n","        print(\"hash is correct\\n\")\n","    else:\n","        print(\"hash in not correct\\n\")\n","        ckpt_valid = False\n","\n","if ckpt_valid:\n","    print(f\"Using ckpt: {ckpt_path}\")\n","\n","def load_model_from_config(config, ckpt, verbose=False, device='cuda', half_precision=True):\n","    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n","    print(f\"Loading model from {ckpt}\")\n","    pl_sd = torch.load(ckpt, map_location=map_location)\n","    if \"global_step\" in pl_sd:\n","        print(f\"Global Step: {pl_sd['global_step']}\")\n","    sd = pl_sd[\"state_dict\"]\n","    model = instantiate_from_config(config.model)\n","    m, u = model.load_state_dict(sd, strict=False)\n","    if len(m) > 0 and verbose:\n","        print(\"missing keys:\")\n","        print(m)\n","    if len(u) > 0 and verbose:\n","        print(\"unexpected keys:\")\n","        print(u)\n","\n","    if half_precision:\n","        model = model.half().to(device)\n","    else:\n","        model = model.to(device)\n","    model.eval()\n","    return model\n","\n","if load_on_run_all and ckpt_valid:\n","    local_config = OmegaConf.load(f\"{ckpt_config_path}\")\n","    model = load_model_from_config(local_config, f\"{ckpt_path}\",half_precision=half_precision)\n","    model = model.to(device)\n","\n","artists = pd.read_csv('/content/EvoGen-Prompt-Evolution/Wordlists/artists.csv')\n","genres = pd.read_csv('/content/EvoGen-Prompt-Evolution/Wordlists/genres.csv')\n","words = pd.read_csv('/content/EvoGen-Prompt-Evolution/Wordlists/wordlist.csv')\n","words_aes = pd.read_csv('/content/EvoGen-Prompt-Evolution/Wordlists/wordsprompt.csv')\n","engrams_aes = pd.read_csv('/content/EvoGen-Prompt-Evolution/Wordlists/engramprompt.csv')\n","\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"I9nFTJVoy0Bu"},"outputs":[],"source":["#@title Aesthetics Helpers\n","\n","from torchvision.transforms import functional as TF\n","import torch.nn.functional as F\n","\n","!git clone https://github.com/openai/CLIP\n","!git clone https://github.com/crowsonkb/simulacra-aesthetic-models\n","!pip install -e ./CLIP\n","import sys\n","sys.path.append('./CLIP')\n","\n","import clip\n","from torchvision import transforms\n","import matplotlib.pyplot as plt \n","\n","class AestheticMeanPredictionLinearModel(nn.Module):\n","    def __init__(self, feats_in):\n","        super().__init__()\n","        self.linear = nn.Linear(feats_in, 1)\n","\n","    def forward(self, input):\n","        x = F.normalize(input, dim=-1) * input.shape[-1] ** 0.5\n","        return self.linear(x)\n","\n","clip_model_name = 'ViT-B/16'\n","clip_model = clip.load(clip_model_name, jit=False, device=device)[0]\n","clip_model.eval().requires_grad_(False)\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                 std=[0.26862954, 0.26130258, 0.27577711])\n","\n","# 512 is embed dimension for ViT-B/16 CLIP\n","aes_model = AestheticMeanPredictionLinearModel(512)\n","aes_model.load_state_dict(\n","    torch.load(\"/content/simulacra-aesthetic-models/models/sac_public_2022_06_29_vit_b_16_linear.pth\")\n",")\n","\n","aes_model = aes_model.to(device)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"te6BzmiZpAyA"},"outputs":[],"source":["#@title Evolution Helpers\n","\n","class PromptGenerator:\n","    def __init__(self, population_count, prompt_length_max, prompt_length_min,\n","                 artist_prop, genre_prop, custom_prop, delete_prop, add_prop, mutate_prop,\n","                 shuffle_prop, cross_prop, k):\n","        self.artists = artists\n","        self.genres = genres\n","        if use_aes_words:\n","          self.words = words_aes\n","        if use_aes_engrams:\n","          self.words = engrams_aes\n","        if use_aes_words and use_aes_engrams:\n","          self.words = words_aes.append(engrams_aes)\n","        if use_aes_words == False and use_aes_engrams == False:\n","          self.words = words\n","\n","        self.custom = custom\n","        self.population_count = population_count\n","        self.prompt_length_max = prompt_length_max \n","        self.prompt_length_min = prompt_length_min\n","\n","        self.artist_prop = artist_prop \n","        self.genre_prop = genre_prop \n","        self.custom_prop = custom_prop\n","\n","        self.word_prop = 1 - self.artist_prop - self.genre_prop\n","        self.delete_prop = delete_prop \n","        self.add_prop = add_prop \n","        \n","        self.mutate_prop = mutate_prop \n","        self.shuffle_prop = shuffle_prop \n","        self.cross_prop = cross_prop \n","        self.k = k \n","\n","        self.fittness_history = []\n","\n","    def initialize_prompt_population(self):\n","        #initialize the prompt population by randomly selecting words from artists, genres, and words dictionaries\n","        prompt_population = []\n","        for i in range(self.population_count):\n","            prompt = []\n","            for j in range(np.random.randint(self.prompt_length_min, self.prompt_length_max)):\n","                #pic based on artist_prop, genre_prop, and word_prop probabilities which dataframe to select from\n","                rand_num = np.random.random()\n","                if rand_num < self.artist_prop:\n","                    prompt.append(self.artists.sample(1).artist.values[0])\n","                elif rand_num < self.artist_prop + self.genre_prop:\n","                    prompt.append(self.genres.sample(1).genre.values[0])\n","                elif rand_num < self.artist_prop + self.genre_prop + self.custom_prop:\n","                    prompt.append(self.custom.sample(1).custom.values[0])\n","                else:\n","                    prompt.append(self.words.sample(1).word.values[0])\n","            prompt_population.append(prompt)\n","        self.prompt_population = prompt_population\n","\n","    def selection(self, scores):\n","        selection_ix = np.random.randint(self.population_count)\n","        for ix in np.random.randint(0, self.population_count, self.k-1):\n","            if scores[ix] > scores[selection_ix]:\n","                selection_ix = ix\n","        return self.prompt_population[selection_ix]\n","\n","    def cross_over(self, prompt_1, prompt_2):\n","        c1, c2 = prompt_1, prompt_2\n","        rand_num = np.random.random()\n","        if rand_num < self.cross_prop:\n","            if len(prompt_2) ==0:\n","              prompt_index = 0\n","            else:\n","              prompt_index = np.random.randint(0, min(len(prompt_1), len(prompt_2)))\n","            c1 = prompt_1[:prompt_index] + prompt_2[prompt_index:]\n","            c2 = prompt_2[:prompt_index] + prompt_1[prompt_index:]\n","        return [c1, c2]\n","\n","    def mutate_prompts(self, prompt):\n","        if (len(prompt) == 0):\n","          prompt_index = 0\n","          rand_num = np.random.random()\n","          if rand_num < self.artist_prop:\n","              prompt.insert(prompt_index, self.artists.sample(1).artist.values[0])\n","          elif rand_num < self.artist_prop + self.genre_prop:\n","              prompt.insert(prompt_index, self.genres.sample(1).genre.values[0])\n","          elif rand_num < self.artist_prop + self.genre_prop + self.custom_prop:\n","              prompt.insert(prompt_index, self.custom.sample(1).custom.values[0])\n","          else:\n","              prompt.insert(prompt_index, self.words.sample(1).word.values[0])\n","\n","        for i in range(len(prompt)):\n","            rand_num = np.random.random()\n","            if rand_num < self.mutate_prop:\n","                rand_num = np.random.random()\n","                if rand_num < self.artist_prop:\n","                    prompt[i] = self.artists.sample(1).artist.values[0]\n","                elif rand_num < self.artist_prop + self.genre_prop:\n","                    prompt[i] = self.genres.sample(1).genre.values[0]\n","                elif rand_num < self.artist_prop + self.genre_prop + self.custom_prop:\n","                    prompt[i] = self.custom.sample(1).custom.values[0]\n","                else:\n","                    prompt[i] = self.words.sample(1).word.values[0]\n","\n","\n","        delete_count = np.random.binomial(len(prompt), self.delete_prop)\n","        if len(prompt) - delete_count < 2:\n","            delete_count = len(prompt) - 2\n","\n","        prompt = np.delete(prompt, np.random.randint(len(prompt), size=delete_count)).tolist()\n","        \n","        for i in range(len(prompt)):\n","            rand_num = np.random.random()\n","            if rand_num < self.add_prop:\n","                rand_num = np.random.random()\n","                if rand_num < self.artist_prop:\n","                    prompt.insert(i, self.artists.sample(1).artist.values[0])\n","                elif rand_num < self.artist_prop + self.genre_prop:\n","                    prompt.insert(i, self.genres.sample(1).genre.values[0])\n","                elif rand_num < self.artist_prop + self.genre_prop + self.custom_prop:\n","                    prompt.insert(i, self.custom.sample(1).custom.values[0])\n","                else:\n","                    prompt.insert(i, self.words.sample(1).word.values[0])\n","            \n","        rand_num = np.random.random()\n","        if rand_num < self.shuffle_prop:\n","            prompt = np.random.permutation(prompt).tolist()\n","            \n","        return prompt\n","\n","    def create_next_generation(self, scores):\n","        selected = [self.selection(scores) for _ in range(self.population_count)]\n","        children = []\n","        for i in range(0, self.population_count, 2):\n","            prompt_1, prompt_2 = selected[i], selected[i+1]\n","            for c in self.cross_over(prompt_1, prompt_2):\n","                c = self.mutate_prompts(c)\n","                children.append(c)\n","\n","        filtered_children = []\n","        for elem in children:\n","            if elem not in filtered_children:\n","                filtered_children.append(elem)\n","\n","        children = filtered_children\n","\n","        missing_prompts = self.population_count - len(children)\n","        print(\"The following number of duplicate prompts had to be replaced with random ones:\"+str(missing_prompts))\n","        for i in range(missing_prompts):\n","            prompt = []\n","            for j in range(np.random.randint(self.prompt_length_min, self.prompt_length_max)):\n","                rand_num = np.random.random()\n","                if rand_num < self.artist_prop:\n","                    prompt.append(self.artists.sample(1).artist.values[0])\n","                elif rand_num < self.artist_prop + self.genre_prop:\n","                    prompt.append(self.genres.sample(1).genre.values[0])\n","                else:\n","                    prompt.append(self.words.sample(1).word.values[0])\n","            children.append(prompt)\n","\n","        self.prompt_population = children\n","\n","    def population_as_string(self):\n","        return [' '.join(prompt) for prompt in self.prompt_population]"]},{"cell_type":"markdown","metadata":{"id":"ov3r4RD1tzsT"},"source":["# Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9Rs04mbN5lO"},"outputs":[],"source":["custom_words = [\"üòÄ\", \"üòÉ\", \"üòÑ\", \"üòÅ\", \"üòÜ\", \"üòÖ\", \"üòÇ\", \"ü§£\", \"ü•≤\", \"‚ò∫Ô∏è\", \"üòä\", \"üòá\", \"üôÇ\", \"üôÉ\", \"üòâ\", \"üòå\", \"üòç\", \"ü•∞\", \"üòò\", \"üòó\"]\n","custom = pd.DataFrame(custom_words, columns=[\"custom\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pCnE04dVsIdi"},"outputs":[],"source":["#@markdown **Evolutionary Algorithm Settings**\n","\n","#@markdown General population settings, such as how many generations the algorithm runs for, how many prompts there are in each generation, and the word length range of the prompts.\n","generations = 25 #@param\n","n_samples = 12 #@param\n","population_count = 100 #@param\n","population_count = int(n_samples*(population_count//n_samples + 1))\n","prompt_length_max = 15 #@param\n","prompt_length_min = 3 #@param\n","#@markdown Probability to sample from one of the word lists when adding or mutating a word. The difference between the sum of the three custom lists and 1 is the probability to sample from the English dictionary word list.\n","artist_prop = 0.04 #@param\n","genre_prop = 0.08 #@param\n","custom_prop = 0.0 #@param\n","#@markdown Decide which list to use if the genre, custom and artists list are not selected from sampling. Use either a list from high scoring prompts, 2/3-grams of those prompts of both. If none are selected use a complete english dictionary.\n","use_aes_words = True #@param {type:\"boolean\"}\n","use_aes_engrams = True #@param {type:\"boolean\"}\n","#@markdown Generation evolution settings including the probability to delete, add and swap out each word for a new one from the dictionary.\n","delete_prop = 0.1 #@param\n","add_prop = 0.1 #@param\n","mutate_prop = 0.2 #@param\n","shuffle_prop = 0.1 #@param\n","#@markdown Generation evolution settings for the new generation parent selection and breeding. The cross-over probability is the probability of the parents swapping prompt parts. K denotes the rounds in the tournament selection process. A higher K value means fewer parents generate the next generation, this means a higher score increase but less diversity in the prompts.\n","cross_prop = 0.8 #@param\n","k = 4 #@param\n","#@markdown Cutoff score to save the image and prompt\n","cutoff = 5.5 #@param\n","\n","prompt_generator = PromptGenerator(population_count, prompt_length_max, prompt_length_min,\n","                                  artist_prop, genre_prop, custom_prop, delete_prop, add_prop, mutate_prop,\n","                                  shuffle_prop, cross_prop, k)\n","prompt_generator.initialize_prompt_population()\n","\n","mean_score = []\n","best_score = []\n","mean_prompt_length = []"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n8LaLQzTtUOr"},"outputs":[],"source":["def DeforumArgs(n_samples):\n","    #@markdown **Image Generation Settings**\n","\n","    #@markdown Image generation settings have an impact on the speed and behavior of the evolutionary algorithm, the euler_a sampler in conjunction a with low step size and resolution is advisable for quick prompt evolution. Good prompts can then later be used to generate higher-quality images. The parameter n_samples determines how many images are generated per prompt, the scores are then averaged. A higher n_samples slows down the generation but stabilizes the optimization.\n","    \n","    #@markdown **Save & Display Settings**\n","    batch_name = \"StableFun\" #@param {type:\"string\"}\n","    outdir = get_output_folder(output_path, batch_name)\n","    save_samples = False #@param {type:\"boolean\"}\n","    display_samples = False #@param {type:\"boolean\"}\n","\n","    #@markdown **Image Settings**\n","    n_samples = n_samples\n","    W = 400 #@param\n","    H = 400 #@param\n","    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n","\n","    #@markdown **Init Settings**\n","    use_init = False #@param {type:\"boolean\"}\n","    strength = 0.5 #@param {type:\"number\"}\n","    init_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n","\n","    #@markdown **Sampling Settings**\n","    seed = -1 #@param\n","    sampler = 'euler_ancestral' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n","    steps = 10 #@param\n","    scale = 7 #@param\n","    ddim_eta = 0.0 #@param\n","    dynamic_threshold = None\n","    static_threshold = None   \n","\n","    #@markdown **Batch Settings**\n","    seed_behavior = \"random\" #@param [\"iter\",\"fixed\",\"random\"]\n","\n","    precision = 'autocast' \n","    fixed_code = True\n","    C = 4\n","    f = 8\n","\n","    prompt = \"\"\n","    timestring = \"\"\n","    init_latent = None\n","    init_sample = None\n","    init_c = None\n","\n","    return locals()\n","\n","args = SimpleNamespace(**DeforumArgs(n_samples))\n","args.timestring = time.strftime('%Y%m%d%H%M%S')\n","args.strength = max(0.0, min(1.0, args.strength))\n","\n","if args.seed == -1:\n","    args.seed = random.randint(0, 2**32)\n","if not args.use_init:\n","    args.init_image = None\n","    args.strength = 0\n","if args.sampler == 'plms' and (args.use_init != 'None'):\n","    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n","    args.sampler = 'klms'\n","if args.sampler != 'ddim':\n","    args.ddim_eta = 0\n","\n","def next_seed(args):\n","    if args.seed_behavior == 'iter':\n","        args.seed += 1\n","    elif args.seed_behavior == 'fixed':\n","        pass # always keep seed the same\n","    else:\n","        args.seed = random.randint(0, 2**32)\n","    return args.seed\n","\n","def render_image_batch(args, prompts):    \n","    # create output folder for the batch\n","    index = 0\n","    \n","    # function for init image batching\n","    init_array = []\n","    if args.use_init:\n","        if args.init_image == \"\":\n","            raise FileNotFoundError(\"No path was given for init_image\")\n","        if args.init_image.startswith('http://') or args.init_image.startswith('https://'):\n","            init_array.append(args.init_image)\n","        elif not os.path.isfile(args.init_image):\n","            if args.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n","                args.init_image += \"/\" \n","            for image in sorted(os.listdir(args.init_image)): # iterates dir and appends images to init_array\n","                if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n","                    init_array.append(args.init_image + image)\n","        else:\n","            init_array.append(args.init_image)\n","    else:\n","        init_array = [\"\"]\n","\n","    all_images = []\n","        \n","    for image in init_array: # iterates the init images\n","        args.init_image = image\n","        results = generate(args, prompts)\n","        for image in results:\n","            all_images.append(image)\n","            index += 1\n","        args.seed = next_seed(args)\n","        \n","    return all_images"]},{"cell_type":"markdown","metadata":{"id":"s8RAo2zI-vQm"},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oGn7-J1TvkBJ"},"outputs":[],"source":["#@title Evolution Loop\n","\n","def plot_fittness_history():\n","  # plot the mean score over time, the best score time, a score histogram, and the mean prompt length over time in a 2 by 2 subplot grid\n","  plt.figure(figsize=(16,10))\n","  plt.rcParams.update({'font.size': 12})\n","  plt.subplot(2,2,1)\n","  plt.plot(mean_score)\n","  plt.title(\"Mean Score\")\n","  plt.xlabel(\"Generation\")\n","  plt.ylabel(\"Score\")\n","  plt.subplot(2,2,2)\n","  plt.plot(best_score)\n","  plt.title(\"Best Score\")\n","  plt.xlabel(\"Generation\")\n","  plt.ylabel(\"Score\")\n","  plt.subplot(2,2,3)\n","  plt.hist(scores, bins=20)\n","  plt.title(\"Score Histogram\")\n","  plt.xlabel(\"Score\")\n","  plt.ylabel(\"Frequency\")\n","  plt.subplot(2,2,4)\n","  plt.plot(mean_prompt_length)\n","  plt.title(\"Mean Prompt Length\")\n","  plt.xlabel(\"Generation\")\n","  plt.ylabel(\"Prompt Length\")\n","  plt.tight_layout()\n","  plt.show()\n","\n","def plot_top_9():\n","  top_9idx = torch.flip(np.argsort(scores)[-9:], (0,)).tolist()\n","  print(*[prompts[i] for i in top_9idx], sep = \"\\n\")\n","  top_9 = image_population[top_9idx]\n","  top_9 = torch.cat([top_9[i:i+3] for i in range(0, 9, 3)], dim=3)\n","  top_9 = torch.cat([top_9[i:i+1] for i in range(0, 3, 1)], dim=2)\n","  best_img = transforms.ToPILImage()(top_9[0])\n","  display(best_img)\n","  best_img.save(os.path.join(gen_path, \"best_9.png\"))\n","\n","def fittness_function(images):\n","  clip_preped_images = torch.zeros(size = (args.n_samples, 3, 224, 224))\n","  for i in range(len(images)):\n","    img = TF.resize(images[i], 224, transforms.InterpolationMode.LANCZOS)\n","    img = TF.center_crop(img, (224,224))\n","    img = TF.to_tensor(img).to(device)\n","    img = normalize(img)\n","    clip_preped_images[i] = img\n","\n","  clip_image_embed = F.normalize(\n","      clip_model.encode_image(clip_preped_images.to(device)).float(),\n","      dim=-1)\n","  scores = aes_model(clip_image_embed).mean(axis = 1)\n","  return scores\n","\n","def images_to_pil(images):\n","  r_images = []\n","  for x_sample in images:\n","    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n","    image = Image.fromarray(x_sample.astype(np.uint8))\n","    r_images.append(image)\n","  return r_images\n","\n","with torch.no_grad():\n","  for i in range(generations):\n","    gen_path = get_output_folder(output_path, args.batch_name)+'/gen_'+str(i)\n","    os.makedirs(gen_path, exist_ok=True)\n","    os.makedirs(gen_path+\"/best\", exist_ok=True)\n","  \n","    prompts = prompt_generator.population_as_string()\n","    image_population = torch.zeros(size = (prompt_generator.population_count, 3, args.H, args.W))\n","    scores = torch.zeros(prompt_generator.population_count)\n","\n","    for j in range(0, prompt_generator.population_count, args.n_samples):\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      images = render_image_batch(args, prompts[j:(j+args.n_samples)])\n","      images_pil = images_to_pil(images)\n","      scores[j:(j+args.n_samples)] = fittness_function(images_pil)\n","\n","      for k in range(len(images)):\n","        image_population[j+k] = images[k]\n","\n","        if args.display_samples:\n","          print(prompts[j+k])\n","          display(images_pil[k])\n","      \n","        if args.save_samples:\n","            filename = prompts[j+k]+\".png\"\n","            images_pil[k].save(os.path.join(gen_path, filename))\n","\n","        if scores[j+k] >= cutoff:\n","          filename_length = min(150, len(prompts[j+k]))\n","          images_pil[k].save(gen_path+'/best/'+prompts[j+k][:filename_length]+'.png')\n","          with open(gen_path+'/best/'+prompts[j+k][:filename_length]+'.txt', 'w') as f:\n","            f.write(prompts[j+k])\n","            f.write('\\n')\n","            f.write(str(args.seed))\n","\n","    clear_output(wait=True)\n","    mean_score.append(scores.mean().item())\n","    best_score.append(max(scores).item())\n","    mean_prompt_length.append(np.mean([len(prompt) for prompt in prompt_generator.prompt_population]))\n","\n","    plot_fittness_history()\n","    plot_top_9()\n","\n","    prompt_generator.create_next_generation(scores)"]},{"cell_type":"markdown","metadata":{"id":"odZds94fXqZ8"},"source":["# Rerun Prompts in higher Quality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8BJDuV-Yqg2"},"outputs":[],"source":["prompts = [\"epiphenomenalism Lovis Corinth carders\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Q9TR_-H3tBhG"},"outputs":[],"source":["def DeforumArgs():\n","    #@markdown **Save & Display Settings**\n","    batch_name = \"StableFun\" #@param {type:\"string\"}\n","    outdir = get_output_folder(output_path, batch_name)\n","    save_settings = True #@param {type:\"boolean\"}\n","    save_samples = False #@param {type:\"boolean\"}\n","    display_samples = True #@param {type:\"boolean\"}\n","\n","    #@markdown **Image Settings**\n","    n_samples = 1 #@param\n","    W = 512 #@param\n","    H = 512 #@param\n","    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n","\n","    #@markdown **Init Settings**\n","    use_init = False #@param {type:\"boolean\"}\n","    strength = 0.5 #@param {type:\"number\"}\n","    init_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n","\n","    #@markdown **Sampling Settings**\n","    seed = -1 #@param\n","    sampler = 'klms' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n","    steps = 75 #@param\n","    scale = 7 #@param\n","    ddim_eta = 0.0 #@param\n","    dynamic_threshold = None\n","    static_threshold = None   \n","\n","    #@markdown **Batch Settings**\n","    n_batch = 1 #@param\n","    seed_behavior = \"iter\" #@param [\"iter\",\"fixed\",\"random\"]\n","\n","    #@markdown **Grid Settings**\n","    make_grid = False #@param {type:\"boolean\"}\n","    grid_rows = 2 #@param \n","\n","    precision = 'autocast' \n","    fixed_code = True\n","    C = 4\n","    f = 8\n","\n","    prompt = \"\"\n","    timestring = \"\"\n","    init_latent = None\n","    init_sample = None\n","    init_c = None\n","\n","    return locals()\n","\n","\n","args_high_res = SimpleNamespace(**DeforumArgs())\n","args_high_res.timestring = time.strftime('%Y%m%d%H%M%S')\n","args_high_res.strength = max(0.0, min(1.0, args_high_res.strength))\n","\n","\n","if args_high_res.seed == -1:\n","    args_high_res.seed = random.randint(0, 2**32)\n","if not args_high_res.use_init:\n","    args_high_res.init_image = None\n","    args_high_res.strength = 0\n","if args_high_res.sampler == 'plms' and (args_high_res.use_init != 'None'):\n","    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n","    args_high_res.sampler = 'klms'\n","if args_high_res.sampler != 'ddim':\n","    args_high_res.ddim_eta = 0\n","\n","def next_seed(args_high_res):\n","    if args_high_res.seed_behavior == 'iter':\n","        args_high_res.seed += 1\n","    elif args_high_res.seed_behavior == 'fixed':\n","        pass # always keep seed the same\n","    else:\n","        args_high_res.seed = random.randint(0, 2**32)\n","    return args_high_res.seed\n","\n","def generate_non_batch(args, return_latent=False, return_sample=False, return_c=False):\n","    seed_everything(args.seed)\n","    os.makedirs(args.outdir, exist_ok=True)\n","\n","    if args.sampler == 'plms':\n","        sampler = PLMSSampler(model)\n","    else:\n","        sampler = DDIMSampler(model)\n","\n","    model_wrap = CompVisDenoiser(model)       \n","    batch_size = args.n_samples\n","    prompt = args.prompt\n","    assert prompt is not None\n","    data = [batch_size * [prompt]]\n","\n","    init_latent = None\n","    if args.init_latent is not None:\n","        init_latent = args.init_latent\n","    elif args.init_sample is not None:\n","        init_latent = model.get_first_stage_encoding(model.encode_first_stage(args.init_sample))\n","    elif args.init_image != None and args.init_image != '':\n","        init_image = load_img(args.init_image, shape=(args.W, args.H)).to(device)\n","        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n","        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space        \n","\n","    sampler.make_schedule(ddim_num_steps=args.steps, ddim_eta=args.ddim_eta, verbose=False)\n","\n","    t_enc = int((1.0-args.strength) * args.steps)\n","\n","    start_code = None\n","    if args.fixed_code and init_latent == None:\n","        start_code = torch.randn([args.n_samples, args.C, args.H // args.f, args.W // args.f], device=device)\n","\n","    callback = make_callback(sampler=args.sampler,\n","                            dynamic_threshold=args.dynamic_threshold, \n","                            static_threshold=args.static_threshold)\n","\n","    results = []\n","    precision_scope = autocast if args.precision == \"autocast\" else nullcontext\n","    with torch.no_grad():\n","        with precision_scope(\"cuda\"):\n","            with model.ema_scope():\n","                for prompts in data:\n","                    uc = None\n","                    if args.scale != 1.0:\n","                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n","                    if isinstance(prompts, tuple):\n","                        prompts = list(prompts)\n","                    c = model.get_learned_conditioning(prompts)\n","\n","                    if args.init_c != None:\n","                        c = args.init_c\n","\n","                    if args.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n","                        samples = sampler_fn(\n","                            c=c, \n","                            uc=uc, \n","                            args=args, \n","                            model_wrap=model_wrap, \n","                            init_latent=init_latent, \n","                            t_enc=t_enc, \n","                            device=device, \n","                            cb=callback)\n","                    else:\n","\n","                        if init_latent != None:\n","                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n","                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.scale,\n","                                                    unconditional_conditioning=uc,)\n","                        else:\n","                            if args.sampler == 'plms' or args.sampler == 'ddim':\n","                                shape = [args.C, args.H // args.f, args.W // args.f]\n","                                samples, _ = sampler.sample(S=args.steps,\n","                                                                conditioning=c,\n","                                                                batch_size=args.n_samples,\n","                                                                shape=shape,\n","                                                                verbose=False,\n","                                                                unconditional_guidance_scale=args.scale,\n","                                                                unconditional_conditioning=uc,\n","                                                                eta=args.ddim_eta,\n","                                                                x_T=start_code,\n","                                                                img_callback=callback)\n","\n","                    if return_latent:\n","                        results.append(samples.clone())\n","\n","                    x_samples = model.decode_first_stage(samples)\n","                    if return_sample:\n","                        results.append(x_samples.clone())\n","\n","                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n","\n","                    if return_c:\n","                        results.append(c.clone())\n","\n","                    for x_sample in x_samples:\n","                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n","                        image = Image.fromarray(x_sample.astype(np.uint8))\n","                        results.append(image)\n","    return results\n","\n","def render_high_quality_image_batch(args_high_res):\n","    args_high_res.prompts = prompts\n","    \n","    # create output folder for the batch\n","    os.makedirs(args_high_res.outdir, exist_ok=True)\n","    if args_high_res.save_settings or args_high_res.save_samples:\n","        print(f\"Saving to {os.path.join(args_high_res.outdir, args_high_res.timestring)}_*\")\n","\n","    # save settings for the batch\n","    if args_high_res.save_settings:\n","        filename = os.path.join(args_high_res.outdir, f\"{args_high_res.timestring}_settings.txt\")\n","        with open(filename, \"w+\", encoding=\"utf-8\") as f:\n","            json.dump(dict(args_high_res.__dict__), f, ensure_ascii=False, indent=4)\n","\n","    index = 0\n","    \n","    # function for init image batching\n","    init_array = []\n","    if args_high_res.use_init:\n","        if args_high_res.init_image == \"\":\n","            raise FileNotFoundError(\"No path was given for init_image\")\n","        if args_high_res.init_image.startswith('http://') or args_high_res.init_image.startswith('https://'):\n","            init_array.append(args_high_res.init_image)\n","        elif not os.path.isfile(args_high_res.init_image):\n","            if args_high_res.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n","                args_high_res.init_image += \"/\" \n","            for image in sorted(os.listdir(args_high_res.init_image)): # iterates dir and appends images to init_array\n","                if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n","                    init_array.append(args_high_res.init_image + image)\n","        else:\n","            init_array.append(args_high_res.init_image)\n","    else:\n","        init_array = [\"\"]\n","\n","    # when doing large batches don't flood browser with images\n","    clear_between_batches = args_high_res.n_batch >= 32\n","\n","    for iprompt, prompt in enumerate(prompts):  \n","        args_high_res.prompt = prompt\n","\n","        all_images = []\n","\n","        for batch_index in range(args_high_res.n_batch):\n","            if clear_between_batches: \n","                display.clear_output(wait=True)            \n","            print(f\"Batch {batch_index+1} of {args_high_res.n_batch}\")\n","            \n","            for image in init_array: # iterates the init images\n","                args_high_res.init_image = image\n","                results = generate_non_batch(args_high_res)\n","                for image in results:\n","                    if args_high_res.make_grid:\n","                        all_images.append(T.functional.pil_to_tensor(image))\n","                    if args_high_res.save_samples:\n","                        filename = f\"{args_high_res.timestring}_{index:05}_{args_high_res.seed}.png\"\n","                        image.save(os.path.join(args_high_res.outdir, filename))\n","                    if args_high_res.display_samples:\n","                        display(image)\n","                    index += 1\n","                args_high_res.seed = next_seed(args_high_res)\n","\n","        #print(len(all_images))\n","        if args_high_res.make_grid:\n","            grid = make_grid(all_images, nrow=int(len(all_images)/args_high_res.grid_rows))\n","            grid = rearrange(grid, 'c h w -> h w c').cpu().numpy()\n","            filename = f\"{args_high_res.timestring}_{iprompt:05d}_grid_{args_high_res.seed}.png\"\n","            grid_image = Image.fromarray(grid.astype(np.uint8))\n","            grid_image.save(os.path.join(args_high_res.outdir, filename))\n","            clear_output(wait=True)            \n","            display(grid_image)\n","\n","render_high_quality_image_batch(args_high_res)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDq6NUe9G0Jb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"private_outputs":true,"provenance":[{"file_id":"https://github.com/MagnusPetersen/EvoGen-Prompt-Evolution/blob/main/Notebook/EvoGen.ipynb","timestamp":1663446110050},{"file_id":"https://github.com/MagnusPetersen/EvoGen-Prompt-Evolution/blob/main/Notebook/EvoGen.ipynb","timestamp":1662414674349},{"file_id":"1x1PuuKmswGv9mjmvA6EDVUipy6bZSMbZ","timestamp":1661954013198},{"file_id":"https://github.com/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb","timestamp":1661936067442}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.5 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.5"},"vscode":{"interpreter":{"hash":"9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"}}},"nbformat":4,"nbformat_minor":0}
